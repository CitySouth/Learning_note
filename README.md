# Learning
# 面试
## Part I: 简历
## Part II: 深度学习
### CNN
#### CNN原理
**选取超参数的正确方法：**  
将原始训练集分为训练集和验证集，在验证集上测试不同的超参数，保留表现最好的那个。如果训练数据量不够，使用交叉验证。以最优超参数在测试集跑且只跑一次。  

**评分函数：**  
原始数据到类别分值的映射。

**损失函数：**  
量化预测分类标签的得分与真实标签之间的一致性，不正确分类的得分与真实标签之间的差值大于某个阈值（通常为1.0），则损失值为0，否则计算损失值(差值+阈值)。  

**最优化：**  
> 梯度计算  
- 数值梯度法 计算简单，结果近似，耗费资源  
- 分析梯度法 计算准确，容易出错
- 实际中 分析梯度法+梯度检查（分析梯度法的结果与数值梯度做比较）
> 梯度下降
- 小批量数据梯度下降
- 随机梯度下降（SGD） 每个批量中只有一个数据样本

**反向传播：**  
利用链式法则递归计算表达式的梯度  

**神经网络：**  
> 数据预处理
- 均值减法  `X -= np.mean(X, axis=0)` 对于图像 `X -= np.mean(X)`
- 归一化(Normalization)  将数据的所有维度都归一化，使其取值范围都近似相等  
    1. 先做零中心化，然后 `X /= np.std(X, axis=0)`
    2. 对每个维度进行归一化处理，使每个维度的最大值和最小值是1和-1
- PCA和白化(Whitening)  先对数据进行零中心化处理，然后计算协方差矩阵  
```python
X -= np.mean(X, axis=0)
cov = np.dot(X.T, X) / X.shape[0]
```
> 权重初始化
`w = np.rando.rand(n) * sqrt(2.0/n)`
- 批量归一化（Batch Normalization）全连接层或卷积层与激活函数之间添加一个BatchNorm层，可以理解为在网络的每一层之前都做预处理
> 正则化 Regularization  通过控制神经网络的容量防止过拟合
---
## Q&A
> Imagenet比赛中兴起的神经网络模型
Resnet、Alexnet、VGG、Inception
